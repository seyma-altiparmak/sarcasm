{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7YLLy4071Xn4yBOGxK2AJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seyma-altiparmak/sarcasm/blob/main/SarcasmRec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sarcasm Recognition**\n",
        "### **used dataset :** kaggle dataset"
      ],
      "metadata": {
        "id": "y8bHoUlFk4a1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bx5QRKyWi3mE",
        "outputId": "2981a6a1-034f-4e4e-de89-fe351a909b28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   label                                            comment     author  \\\n",
            "0      0                                         NC and NH.  Trumpbart   \n",
            "1      0  You do know west teams play against west teams...  Shbshb906   \n",
            "2      0  They were underdogs earlier today, but since G...   Creepeth   \n",
            "3      0  This meme isn't funny none of the \"new york ni...  icebrotha   \n",
            "4      0                    I could use one of those tools.  cush2push   \n",
            "\n",
            "            subreddit  score  ups  downs     date          created_utc  \\\n",
            "0            politics      2   -1     -1  2016-10  2016-10-16 23:55:23   \n",
            "1                 nba     -4   -1     -1  2016-11  2016-11-01 00:24:10   \n",
            "2                 nfl      3    3      0  2016-09  2016-09-22 21:45:37   \n",
            "3  BlackPeopleTwitter     -8   -1     -1  2016-10  2016-10-18 21:03:47   \n",
            "4  MaddenUltimateTeam      6   -1     -1  2016-12  2016-12-30 17:00:13   \n",
            "\n",
            "                                      parent_comment  \n",
            "0  Yeah, I get that argument. At this point, I'd ...  \n",
            "1  The blazers and Mavericks (The wests 5 and 6 s...  \n",
            "2                            They're favored to win.  \n",
            "3                         deadass don't kill my buzz  \n",
            "4  Yep can confirm I saw the tool they use for th...  \n",
            "           7u1ht                                            c07em3g  \\\n",
            "0          7u92p  c07f8sq c07flil c07fgh5 c07f9ck c07fn0s c07fl0...   \n",
            "1          7vvpw            c07khcb c07jx0k c07jzxi c07k0t3 c07jx2j   \n",
            "2  7vv27 c07ju7y                                            c07jub3   \n",
            "3          7wco4                            c07l718 c07lfcd c07l7zu   \n",
            "4          7xgdr                                            c07o9qu   \n",
            "\n",
            "                       1  \n",
            "0  0 0 0 0 0 0 0 0 0 0 0  \n",
            "1              0 0 0 0 0  \n",
            "2                      1  \n",
            "3                  0 0 0  \n",
            "4                      1  \n",
            "   7x7dx  c07nkao c07nk63  1 0\n",
            "0  7xtxk  c07pji4 c07ppds  1 0\n",
            "1  7zh5l  c07twag c07twp2  1 0\n",
            "2  80nmo  c07y1rj c07xhyn  0 1\n",
            "3  8139p  c07yhlm c07yoiw  1 0\n",
            "4  bnmod  c0nnujf c0nofrs  1 0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load datasets\n",
        "train_df = pd.read_csv('/content/context/train-balanced-sarcasm.csv')\n",
        "unbalanced_test_df = pd.read_csv('/content/context/test-unbalanced.csv')\n",
        "balanced_test_df = pd.read_csv('/content/context/test-balanced.csv')\n",
        "\n",
        "# Check the first few rows to understand the structure of the datasets\n",
        "print(train_df.head())\n",
        "print(unbalanced_test_df.head())\n",
        "print(balanced_test_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "\n",
        "# Download stopwords if not already done\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Assuming you're reading from a CSV file\n",
        "df = pd.read_csv('/content/context/train-balanced-sarcasm.csv')\n",
        "\n",
        "# Define the text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):  # Check if the text is not a string (e.g., NaN)\n",
        "        return ''  # Return an empty string if the text is not a valid string\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))  # Set of English stopwords\n",
        "    words = text.split()  # Split the text into words\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]  # Remove stopwords\n",
        "    return ' '.join(filtered_words)  # Join the words back into a single string\n",
        "\n",
        "# Ensure that any NaN or non-string values in the 'comment' column are handled\n",
        "df['comment'] = df['comment'].fillna('')  # Replace NaN values with an empty string\n",
        "\n",
        "# Apply the preprocessing function to the 'comment' column\n",
        "df['processed_text'] = df['comment'].apply(preprocess_text)\n",
        "\n",
        "# Check the first few rows of the DataFrame to verify the changes\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRnDqLrLjl92",
        "outputId": "a31af3d5-f35b-4639-e616-18a36c181cb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   label                                            comment     author  \\\n",
            "0      0                                         NC and NH.  Trumpbart   \n",
            "1      0  You do know west teams play against west teams...  Shbshb906   \n",
            "2      0  They were underdogs earlier today, but since G...   Creepeth   \n",
            "3      0  This meme isn't funny none of the \"new york ni...  icebrotha   \n",
            "4      0                    I could use one of those tools.  cush2push   \n",
            "\n",
            "            subreddit  score  ups  downs     date          created_utc  \\\n",
            "0            politics      2   -1     -1  2016-10  2016-10-16 23:55:23   \n",
            "1                 nba     -4   -1     -1  2016-11  2016-11-01 00:24:10   \n",
            "2                 nfl      3    3      0  2016-09  2016-09-22 21:45:37   \n",
            "3  BlackPeopleTwitter     -8   -1     -1  2016-10  2016-10-18 21:03:47   \n",
            "4  MaddenUltimateTeam      6   -1     -1  2016-12  2016-12-30 17:00:13   \n",
            "\n",
            "                                      parent_comment  \\\n",
            "0  Yeah, I get that argument. At this point, I'd ...   \n",
            "1  The blazers and Mavericks (The wests 5 and 6 s...   \n",
            "2                            They're favored to win.   \n",
            "3                         deadass don't kill my buzz   \n",
            "4  Yep can confirm I saw the tool they use for th...   \n",
            "\n",
            "                                      processed_text  \n",
            "0                                             NC NH.  \n",
            "1  know west teams play west teams east teams right?  \n",
            "2  underdogs earlier today, since Gronk's announc...  \n",
            "3         meme funny none \"new york nigga\" ones are.  \n",
            "4                               could use one tools.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZ0j3OTXjttf",
        "outputId": "08d0af21-4a3e-4c4e-a6a6-47b03ef7abf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['label', 'comment', 'author', 'subreddit', 'score', 'ups', 'downs',\n",
            "       'date', 'created_utc', 'parent_comment', 'processed_text'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try loading the test datasets again with no headers\n",
        "balanced_test_df = pd.read_csv('/content/context/test-balanced.csv', header=None)  # Update with the correct path\n",
        "unbalanced_test_df = pd.read_csv('/content/context/test-unbalanced.csv', header=None)  # Update with the correct path\n",
        "\n",
        "# Verify the columns\n",
        "print(\"Columns in balanced_test_df:\", balanced_test_df.columns)\n",
        "print(\"Columns in unbalanced_test_df:\", unbalanced_test_df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7GKA4jljxnc",
        "outputId": "456d95c6-9e1c-4185-d2c7-db31971aedb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in balanced_test_df: Index([0, 1, 2], dtype='int64')\n",
            "Columns in unbalanced_test_df: Index([0, 1, 2], dtype='int64')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the first few rows of preprocessed text to ensure it contains meaningful words\n",
        "train_df['comment'] = train_df['comment'].fillna('')\n",
        "train_df['processed_text'] = train_df['comment'].apply(preprocess_text)\n",
        "print(\"\\nSample preprocessed train data (first 5 rows):\")\n",
        "print(train_df['processed_text'].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPbRydjtj_SA",
        "outputId": "89abcfb4-ba24-4938-cc0e-d6620edbb3c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample preprocessed train data (first 5 rows):\n",
            "0                                               NC NH.\n",
            "1    know west teams play west teams east teams right?\n",
            "2    underdogs earlier today, since Gronk's announc...\n",
            "3           meme funny none \"new york nigga\" ones are.\n",
            "4                                 could use one tools.\n",
            "Name: processed_text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Read the datasets without headers (header=None) and manually assign column names\n",
        "train_df = pd.read_csv('/content/context/train-balanced-sarcasm.csv')  # Make sure you are providing the correct path\n",
        "\n",
        "# Assuming your balanced_test and unbalanced_test have only 3 columns each:\n",
        "balanced_test_df = pd.read_csv('/content/context/test-balanced.csv', header=None)\n",
        "unbalanced_test_df = pd.read_csv('/content/context/test-unbalanced.csv', header=None)\n",
        "\n",
        "# Assign appropriate column names based on the actual number of columns in each DataFrame\n",
        "# For the training data, which has 9 columns (adjust based on your data)\n",
        "train_df.columns = ['label', 'comment', 'author', 'subreddit', 'score', 'ups', 'downs', 'date', 'created_utc','sub']\n",
        "\n",
        "# For the test data, assuming they only have 3 columns, we should assign appropriate names\n",
        "# If the test files only have 'comment', 'label', and one other column (such as 'author' or 'date'), adjust accordingly\n",
        "balanced_test_df.columns = ['comment', 'label', 'extra_column']  # Adjust 'extra_column' if necessary\n",
        "unbalanced_test_df.columns = ['comment', 'label', 'extra_column']  # Adjust 'extra_column' if necessary\n",
        "\n",
        "# Fill NaN values with empty strings in the 'comment' column\n",
        "train_df['comment'] = train_df['comment'].fillna('')  # Replace NaN with empty strings\n",
        "balanced_test_df['comment'] = balanced_test_df['comment'].fillna('')  # Replace NaN with empty strings\n",
        "unbalanced_test_df['comment'] = unbalanced_test_df['comment'].fillna('')  # Replace NaN with empty strings\n",
        "\n",
        "# Apply preprocessing (assuming preprocess_text is already defined)\n",
        "train_df['processed_text'] = train_df['comment'].apply(preprocess_text)\n",
        "balanced_test_df['processed_text'] = balanced_test_df['comment'].apply(preprocess_text)\n",
        "unbalanced_test_df['processed_text'] = unbalanced_test_df['comment'].apply(preprocess_text)\n",
        "\n",
        "# Strip any extra whitespace from column names\n",
        "train_df.columns = train_df.columns.str.strip()\n",
        "balanced_test_df.columns = balanced_test_df.columns.str.strip()\n",
        "unbalanced_test_df.columns = unbalanced_test_df.columns.str.strip()\n",
        "\n",
        "# Verify the columns to ensure 'processed_text' exists in each DataFrame\n",
        "print(\"Columns in train_df:\", train_df.columns)\n",
        "print(\"Columns in balanced_test_df:\", balanced_test_df.columns)\n",
        "print(\"Columns in unbalanced_test_df:\", unbalanced_test_df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNbU48V8kALN",
        "outputId": "7fddb178-8eb4-48f8-a48c-d79f2167faf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in train_df: Index(['label', 'comment', 'author', 'subreddit', 'score', 'ups', 'downs',\n",
            "       'date', 'created_utc', 'sub', 'processed_text'],\n",
            "      dtype='object')\n",
            "Columns in balanced_test_df: Index(['comment', 'label', 'extra_column', 'processed_text'], dtype='object')\n",
            "Columns in unbalanced_test_df: Index(['comment', 'label', 'extra_column', 'processed_text'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure 'processed_text' exists in all dataframes\n",
        "assert 'processed_text' in train_df.columns, \"'processed_text' not found in train_df\"\n",
        "assert 'processed_text' in balanced_test_df.columns, \"'processed_text' not found in balanced_test_df\"\n",
        "assert 'processed_text' in unbalanced_test_df.columns, \"'processed_text' not found in unbalanced_test_df\"\n",
        "\n",
        "# Define features and labels\n",
        "X_train = train_df['processed_text']  # Use 'processed_text' for features\n",
        "y_train = train_df['label']\n",
        "\n",
        "X_balanced_test = balanced_test_df['processed_text']\n",
        "y_balanced_test = balanced_test_df['label']\n",
        "\n",
        "X_unbalanced_test = unbalanced_test_df['processed_text']\n",
        "y_unbalanced_test = unbalanced_test_df['label']\n",
        "\n",
        "# Initialize the TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "# Fit and transform the training data\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data (balanced and unbalanced) using the already fitted vectorizer\n",
        "X_balanced_test_tfidf = vectorizer.transform(X_balanced_test)\n",
        "X_unbalanced_test_tfidf = vectorizer.transform(X_unbalanced_test)\n",
        "\n",
        "# Verify the resulting shapes\n",
        "print(\"Train TF-IDF shape:\", X_train_tfidf.shape)\n",
        "print(\"Balanced Test TF-IDF shape:\", X_balanced_test_tfidf.shape)\n",
        "print(\"Unbalanced Test TF-IDF shape:\", X_unbalanced_test_tfidf.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vud-IG9HkNbm",
        "outputId": "32e857b8-0642-4db3-ca24-0a7aa0120f3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train TF-IDF shape: (8320, 12172)\n",
            "Balanced Test TF-IDF shape: (32333, 12172)\n",
            "Unbalanced Test TF-IDF shape: (7152, 12172)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Sample sarcastic vocabulary\n",
        "Vocabulary = [\n",
        "    \"great\", \"awesome\", \"fantastic\", \"perfect\", \"unbelievable\", \"of course\", \"totally\", \"sure\",\n",
        "    \"definitely\", \"thank you\", \"appreciate\", \"thanks so much\", \"ridiculous\", \"insane\", \"good job\",\n",
        "    \"well done\", \"nice work\", \"really\", \"seriously\", \"wow\", \"amazing\", \"beautiful\", \"gorgeous\",\n",
        "    \"just what I needed\", \"how original\", \"what a surprise\", \"yeah right\", \"oh really\",\n",
        "    \"so much fun\", \"oh that makes sense\", \"just great\", \"couldn't have been worse\", \"good for you\",\n",
        "    \"nice one\", \"so thoughtful\", \"so considerate\", \"well, that's just perfect\", \"can't wait\",\n",
        "    \"yeah, totally\", \"sure, why not\", \"I’m sure that’s going to be a huge success\", \"thanks for nothing\",\n",
        "    \"you’re a genius\", \"I’m so glad\", \"you shouldn’t have\", \"how nice of you\", \"thank you so much\",\n",
        "    \"oh, I just love waiting\", \"yeah, sure\", \"thanks for the help\", \"this is fun\", \"oh, this is great\",\n",
        "    \"I can’t wait for this to be over\", \"oh sure, I’m so excited\", \"what a pleasant surprise\", \"no problem\",\n",
        "    \"what a treat\", \"wow, amazing\", \"how thoughtful\", \"you've outdone yourself\", \"I can't believe it\"\n",
        "]\n",
        "\n",
        "# Example dataset\n",
        "train_df = pd.DataFrame({\n",
        "    'comment': [\n",
        "        \"Wow, great job on that!\",\n",
        "        \"Oh, awesome, just what I needed!\",\n",
        "        \"You're such a genius, really!\",\n",
        "        \"Yeah, that was fantastic... not.\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Apply the TfidfVectorizer normally (without custom tokenizer)\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(train_df['comment'])\n",
        "\n",
        "# Get feature names (words)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print the transformed matrix (TF-IDF scores)\n",
        "print(\"\\nTF-IDF Matrix:\")\n",
        "print(X.toarray())\n",
        "\n",
        "# Extract words from vocabulary that are also in the sarcastic Vocabulary list\n",
        "sarcastic_columns = [i for i, word in enumerate(feature_names) if word in Vocabulary]\n",
        "\n",
        "# Get the sarcastic words from the TF-IDF matrix\n",
        "sarcastic_words = [feature_names[i] for i in sarcastic_columns]\n",
        "\n",
        "print(\"\\nSarcastic words found in the comments:\")\n",
        "print(sarcastic_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2zPaoXHkUsy",
        "outputId": "6c504939-99de-4619-a756-271a61c1e5d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF Matrix:\n",
            "[[0.         0.         0.         0.57735027 0.57735027 0.\n",
            "  0.         0.         0.         0.57735027 0.        ]\n",
            " [0.5        0.         0.         0.         0.         0.5\n",
            "  0.5        0.5        0.         0.         0.        ]\n",
            " [0.         0.         0.70710678 0.         0.         0.\n",
            "  0.         0.         0.70710678 0.         0.        ]\n",
            " [0.         0.70710678 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.70710678]]\n",
            "\n",
            "Sarcastic words found in the comments:\n",
            "['awesome', 'fantastic', 'great', 'really', 'wow']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data (balanced and unbalanced) using the already fitted vectorizer\n",
        "X_balanced_test_tfidf = vectorizer.transform(X_balanced_test)\n",
        "X_unbalanced_test_tfidf = vectorizer.transform(X_unbalanced_test)\n",
        "\n",
        "# Verify the resulting shapes\n",
        "print(\"Train TF-IDF shape:\", X_train_tfidf.shape)\n",
        "print(\"Balanced Test TF-IDF shape:\", X_balanced_test_tfidf.shape)\n",
        "print(\"Unbalanced Test TF-IDF shape:\", X_unbalanced_test_tfidf.shape)\n",
        "\n",
        "# Optional: Check the first few rows of the transformed data\n",
        "# For large datasets, printing the entire matrix may not be ideal, but you can view a few sample rows\n",
        "print(\"\\nSample transformed train data (first 5 rows):\")\n",
        "\n",
        "# Convert the sparse matrix to a dense format for easier inspection (only the first 5 rows)\n",
        "sample_train_data = X_train_tfidf[:5].todense()\n",
        "\n",
        "# Convert to a DataFrame to display the words associated with each feature\n",
        "sample_train_df = pd.DataFrame(sample_train_data, columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Display the first 5 rows of the transformed training data\n",
        "print(sample_train_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vs-uPR6SkYEE",
        "outputId": "1f4829de-4626-4c67-da3b-ec9609db8c93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train TF-IDF shape: (8320, 11911)\n",
            "Balanced Test TF-IDF shape: (32333, 11911)\n",
            "Unbalanced Test TF-IDF shape: (7152, 11911)\n",
            "\n",
            "Sample transformed train data (first 5 rows):\n",
            "    00  000  0161   02   03  0300   05   06   08   10  ...  zombie  zombies  \\\n",
            "0  0.0  0.0   0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...     0.0      0.0   \n",
            "1  0.0  0.0   0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...     0.0      0.0   \n",
            "2  0.0  0.0   0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...     0.0      0.0   \n",
            "3  0.0  0.0   0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...     0.0      0.0   \n",
            "4  0.0  0.0   0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...     0.0      0.0   \n",
            "\n",
            "   zone  zoning  zork  zoth   zs  zuljin  zurdos  zwijgen  \n",
            "0   0.0     0.0   0.0   0.0  0.0     0.0     0.0      0.0  \n",
            "1   0.0     0.0   0.0   0.0  0.0     0.0     0.0      0.0  \n",
            "2   0.0     0.0   0.0   0.0  0.0     0.0     0.0      0.0  \n",
            "3   0.0     0.0   0.0   0.0  0.0     0.0     0.0      0.0  \n",
            "4   0.0     0.0   0.0   0.0  0.0     0.0     0.0      0.0  \n",
            "\n",
            "[5 rows x 11911 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ],
      "metadata": {
        "id": "llgOjqpakb76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "train_df = pd.read_csv('/content/context/train-balanced-sarcasm.csv')\n",
        "\n",
        "train_df.columns = ['label', 'comment', 'author', 'subreddit', 'score', 'ups', 'downs', 'date', 'created_utc','sub']\n",
        "# Assuming 'processed_text' is the feature and 'label' is the target\n",
        "X = train_df['comment']  # This should be the processed text (after vectorization)\n",
        "y = train_df['label']  # Assuming there's a 'label' column for sarcasm (0 or 1)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "xKz_dNg6kcqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Check for NaN values in the training and testing data\n",
        "print(X_train.isnull().sum())  # For training data\n",
        "print(X_test.isnull().sum())   # For testing data\n",
        "# Drop rows with NaN values in the 'processed_text' column\n",
        "X_train = X_train.dropna()\n",
        "X_test = X_test.dropna()\n",
        "\n",
        "# Ensure that the corresponding target labels (y_train, y_test) are also filtered accordingly\n",
        "y_train = y_train[X_train.index]\n",
        "y_test = y_test[X_test.index]\n",
        "\n",
        "# Fill NaN values with an empty string\n",
        "X_train = X_train.fillna('')\n",
        "X_test = X_test.fillna('')\n",
        "\n",
        "# Fit and transform the training data, and transform the test data\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_vec, y_train)\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test_vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7SfUC31kgam",
        "outputId": "e920a779-ccf6-44c3-9cc0-4e83880febf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(f'Confusion Matrix:\\n{cm}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdxWwG4SkiTv",
        "outputId": "95a14948-5343-4cca-8c23-797090c62f07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 71.94%\n",
            "Confusion Matrix:\n",
            "[[1091   67]\n",
            " [ 400  106]]\n"
          ]
        }
      ]
    }
  ]
}